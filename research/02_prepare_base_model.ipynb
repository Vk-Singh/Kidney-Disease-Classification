{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.17.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting dagshub\n",
      "  Downloading dagshub-0.3.41-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mlflow-skinny==2.17.2 (from mlflow)\n",
      "  Downloading mlflow_skinny-2.17.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Using cached flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<3 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (16.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (1.5.1)\n",
      "Requirement already satisfied: scipy<2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (1.13.1)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow) (3.1.4)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (8.1.7)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading databricks_sdk-0.36.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (3.1.43)\n",
      "Collecting importlib-metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<25 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (24.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (4.25.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting appdirs>=1.4.4 (from dagshub)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub) (0.27.0)\n",
      "Requirement already satisfied: rich>=13.1.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub) (13.7.1)\n",
      "Collecting dacite~=1.6.0 (from dagshub)\n",
      "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub) (8.2.3)\n",
      "Collecting gql[requests] (from dagshub)\n",
      "  Downloading gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting dataclasses-json (from dagshub)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting treelib>=1.6.4 (from dagshub)\n",
      "  Downloading treelib-1.7.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
      "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub) (2.9.0.post0)\n",
      "Collecting boto3 (from dagshub)\n",
      "  Downloading boto3-1.35.54-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting dagshub-annotation-converter>=0.1.0 (from dagshub)\n",
      "  Downloading dagshub_annotation_converter-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.11.0)\n",
      "Collecting lxml (from dagshub-annotation-converter>=0.1.0->dagshub)\n",
      "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pillow in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (2.9.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<4->mlflow)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from Flask<4->mlflow) (1.6.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow) (4.0.7)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: anyio in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (4.6.2)\n",
      "Requirement already satisfied: certifi in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from python-dateutil->dagshub) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.54 (from boto3->dagshub)\n",
      "  Downloading botocore-1.35.54-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->dagshub)\n",
      "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting yarl<2.0,>=1.6 (from gql[requests]->dagshub)\n",
      "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests-toolbelt<2,>=1.0.0 (from gql[requests]->dagshub)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->dagshub) (1.2.0)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow) (4.0.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.0->dagshub) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.0->dagshub) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow) (3.3.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting multidict>=4.0 (from yarl<2.0,>=1.6->gql[requests]->dagshub)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.6->gql[requests]->dagshub)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.17.2-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dagshub-0.3.41-py3-none-any.whl (251 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading dagshub_annotation_converter-0.1.2-py3-none-any.whl (33 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Downloading graphene-3.4.1-py2.py3-none-any.whl (114 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hDownloading treelib-1.7.0-py3-none-any.whl (18 kB)\n",
      "Downloading boto3-1.35.54-py3-none-any.whl (139 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.35.54-py3-none-any.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading databricks_sdk-0.36.0-py3-none-any.whl (569 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Downloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
      "Using cached sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading werkzeug-3.1.2-py3-none-any.whl (224 kB)\n",
      "Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "Downloading gql-3.5.0-py2.py3-none-any.whl (74 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "Using cached zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: appdirs, zipp, wrapt, Werkzeug, treelib, sqlparse, pyasn1, propcache, pathvalidate, mypy-extensions, multidict, marshmallow, markdown, Mako, lxml, jmespath, itsdangerous, gunicorn, greenlet, graphql-core, dacite, cloudpickle, backoff, yarl, typing-inspect, sqlalchemy, rsa, requests-toolbelt, pyasn1-modules, importlib-metadata, graphql-relay, Flask, docker, deprecated, botocore, s3transfer, opentelemetry-api, graphene, gql, google-auth, dataclasses-json, dagshub-annotation-converter, alembic, opentelemetry-semantic-conventions, databricks-sdk, boto3, opentelemetry-sdk, dagshub, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-3.0.3 Mako-1.3.6 Werkzeug-3.1.2 alembic-1.14.0 appdirs-1.4.4 backoff-2.2.1 boto3-1.35.54 botocore-1.35.54 cloudpickle-3.1.0 dacite-1.6.0 dagshub-0.3.41 dagshub-annotation-converter-0.1.2 databricks-sdk-0.36.0 dataclasses-json-0.6.7 deprecated-1.2.14 docker-7.1.0 google-auth-2.35.0 gql-3.5.0 graphene-3.4.1 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-8.4.0 itsdangerous-2.2.0 jmespath-1.0.1 lxml-5.3.0 markdown-3.7 marshmallow-3.23.1 mlflow-2.17.2 mlflow-skinny-2.17.2 multidict-6.1.0 mypy-extensions-1.0.0 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 pathvalidate-3.2.1 propcache-0.2.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-toolbelt-1.0.0 rsa-4.9 s3transfer-0.10.3 sqlalchemy-2.0.36 sqlparse-0.5.1 treelib-1.7.0 typing-inspect-0.9.0 wrapt-1.16.0 yarl-1.17.1 zipp-3.20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150482352c234b0c8d7d30e5202a0e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=09103e76-e40b-4955-bcc3-a30797be3cff&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=ca335d9c7aec9c9df4c795e9809c19f8a6df4af04d79d1657d60b417a30521ea\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:04:17,021: INFO: _client: HTTP Request: POST https://dagshub.com/login/oauth/middleman \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:04:17,186: INFO: _client: HTTP Request: POST https://dagshub.com/login/oauth/access_token \"HTTP/1.1 200 OK\"]\n",
      "[2024-11-05 01:04:17,340: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as vicky5848d\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as vicky5848d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:04:17,344: INFO: helpers: Accessing as vicky5848d]\n",
      "[2024-11-05 01:04:17,537: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/vicky5848d/Kidney-Disease-Classification \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"vicky5848d/Kidney-Disease-Classification\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"vicky5848d/Kidney-Disease-Classification\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:04:17,542: INFO: helpers: Initialized MLflow to track repo \"vicky5848d/Kidney-Disease-Classification\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository vicky5848d/Kidney-Disease-Classification initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository vicky5848d/Kidney-Disease-Classification initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:04:17,544: INFO: helpers: Repository vicky5848d/Kidney-Disease-Classification initialized!]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='vicky5848d', repo_name='Kidney-Disease-Classification', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_include_top: bool\n",
    "    params_weights: str\n",
    "    params_classes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "\n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        prepare_base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            updated_base_model_path=Path(config.updated_base_model_path),\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_learning_rate=self.params.LEARNING_RATE,\n",
    "            params_include_top=self.params.INCLUDE_TOP,\n",
    "            params_weights=self.params.WEIGHTS,\n",
    "            params_classes=self.params.CLASSES\n",
    "        )\n",
    "\n",
    "        return prepare_base_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from torcheval) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torcheval\n",
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torcheval.metrics import MulticlassAUROC\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_max: 500\n",
      "checkpoint_path: null\n",
      "device: cpu\n",
      "epochs: 50\n",
      "fold: 0\n",
      "img_size: 384\n",
      "learning_rate: 0.001\n",
      "min_lr: 1.0e-08\n",
      "model_name: tf_efficientnet_b0_ns\n",
      "n_accumulate: 3\n",
      "n_fold: 5\n",
      "scheduler: CosineAnnealingLR\n",
      "seed: 42\n",
      "train_batch_size: 12\n",
      "valid_batch_size: 10\n",
      "weight_decay: 1.0e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 50,\n",
    "    \"img_size\": 384,\n",
    "    \"model_name\": \"tf_efficientnet_b0_ns\",\n",
    "    \"checkpoint_path\" : None,\n",
    "    \"train_batch_size\": 12,\n",
    "    \"valid_batch_size\": 10,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"scheduler\": 'CosineAnnealingLR',\n",
    "    \"min_lr\": 1e-8,\n",
    "    \"T_max\": 500,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"fold\" : 0,\n",
    "    \"n_fold\": 5,\n",
    "    \"n_accumulate\": 3,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "import yaml\n",
    "print(yaml.dump(CONFIG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path= \"/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main/artifacts/data_ingestion/kidneyData.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df_cysts = df[df[\"target\"] == 0].reset_index(drop=True)\n",
    "df_normal = df[df[\"target\"] == 1].reset_index(drop=True)\n",
    "df_stone = df[df[\"target\"] == 2].reset_index(drop=True)\n",
    "df_tumor = df[df[\"target\"] == 3].reset_index(drop=True)\n",
    "\n",
    "df_target = df[\"target\"].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "    df, df_target, test_size=0.33, random_state=int(os.environ['PYTHONHASHSEED']))\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "\n",
    "    X_train, y_train, test_size=0.25, random_state=int(os.environ['PYTHONHASHSEED']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5077, 6), (2283, 6), (1377, 6), (3709, 6))"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal.shape, df_tumor.shape, df_stone.shape, df_cysts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6253, 6), (4108, 6), (6253,), (4108,))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mA\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         A\u001b[38;5;241m.\u001b[39mResize(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m], CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      4\u001b[0m         A\u001b[38;5;241m.\u001b[39mRandomRotate90(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      5\u001b[0m         A\u001b[38;5;241m.\u001b[39mFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      6\u001b[0m         A\u001b[38;5;241m.\u001b[39mDownscale(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m),\n\u001b[1;32m      7\u001b[0m         A\u001b[38;5;241m.\u001b[39mShiftScaleRotate(shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m      8\u001b[0m                            scale_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, \n\u001b[1;32m      9\u001b[0m                            rotate_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, \n\u001b[1;32m     10\u001b[0m                            p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     11\u001b[0m         A\u001b[38;5;241m.\u001b[39mHueSaturationValue(\n\u001b[1;32m     12\u001b[0m                 hue_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m     13\u001b[0m                 sat_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m     14\u001b[0m                 val_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m     15\u001b[0m                 p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     16\u001b[0m             ),\n\u001b[1;32m     17\u001b[0m         A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(\n\u001b[1;32m     18\u001b[0m                 brightness_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.1\u001b[39m), \n\u001b[1;32m     19\u001b[0m                 contrast_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m), \n\u001b[1;32m     20\u001b[0m                 p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     21\u001b[0m             ),\n\u001b[1;32m     22\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m     23\u001b[0m                 mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], \n\u001b[1;32m     24\u001b[0m                 std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m], \n\u001b[1;32m     25\u001b[0m                 max_pixel_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255.0\u001b[39m, \n\u001b[1;32m     26\u001b[0m                 p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     27\u001b[0m             ),\n\u001b[1;32m     28\u001b[0m         ToTensorV2()], p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m),\n\u001b[1;32m     29\u001b[0m     \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     31\u001b[0m         A\u001b[38;5;241m.\u001b[39mResize(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m], CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     32\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m     33\u001b[0m                 mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], \n\u001b[1;32m     34\u001b[0m                 std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m], \n\u001b[1;32m     35\u001b[0m                 max_pixel_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255.0\u001b[39m, \n\u001b[1;32m     36\u001b[0m                 p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     37\u001b[0m             ),\n\u001b[1;32m     38\u001b[0m         ToTensorV2()], p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m     39\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Downscale(p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.15, \n",
    "                           rotate_limit=60, \n",
    "                           p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA LOADER\n",
    "class KidneyDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['path'].values   \n",
    "        self.targets = df['target'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img_path = \"/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main/artifacts/data_ingestion/\" + img_path\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'target': target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m yaml\u001b[38;5;241m.\u001b[39mdump(\u001b[43mdata_transforms\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranform.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "yaml.dump(data_transforms, \"tranform.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = KidneyDataset(X_train, transforms=data_transforms[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -1.7240, -1.5185, -1.1075],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -1.6042, -1.3815, -1.0562],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -1.3130, -1.2274, -1.1075],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -1.6331, -1.4230, -1.0028],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -1.5105, -1.2829, -0.9503],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -1.2129, -1.1253, -1.0028],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.4036, -1.1944, -0.7761],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.2816, -1.0550, -0.7238],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -0.9853, -0.8981, -0.7761],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference : https://amaarora.github.io/posts/2020-08-30-gempool.html\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:15:09,222: INFO: _builder: Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-05 01:15:33,472: INFO: _hub: [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.]\n"
     ]
    }
   ],
   "source": [
    "class EffNetModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=4, pretrained=True, checkpoint_path=None):\n",
    "        super(EffNetModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n",
    "\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        output = self.sigmoid(self.linear(pooled_features))\n",
    "        return output\n",
    "\n",
    "    \n",
    "model = EffNetModel(CONFIG['model_name'], checkpoint_path=CONFIG['checkpoint_path'])\n",
    "model.to(CONFIG['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]), torch.Size([3]))"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc  = 0.0\n",
    "    num_batches=len(dataloader)\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader),position=0, leave=True)\n",
    "    for batch, data in bar:\n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        outputs = model(images)\n",
    "       \n",
    "        loss = criterion(outputs, targets)\n",
    "            \n",
    "        loss.backward()\n",
    "    \n",
    "        if (batch + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        multi_auroc = MulticlassAUROC(num_classes=4)\n",
    "        multi_auroc.update(input=outputs, target=targets)\n",
    "        auroc = multi_auroc.compute()\n",
    "\n",
    "              \n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_auroc  += (auroc * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_auroc = running_auroc / dataset_size\n",
    "        if batch % 25 == 0:\n",
    "            loss, current = epoch_loss, batch\n",
    "            step = (batch // 25) + epoch*num_batches//25\n",
    "            mlflow.log_metric(\"loss\", f\"{loss:3f}\", step=(step))\n",
    "            mlflow.log_metric(\"MulticlassAUROC\", f\"{epoch_auroc:3f}\", step=(step))\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, Train_Auroc=epoch_auroc,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def valid_one_epoch(model,optimizer, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "    \n",
    "   \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader), position=0, leave=True)\n",
    "    for batch, data in bar:\n",
    "       \n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        multi_auroc = MulticlassAUROC(num_classes=4)\n",
    "        multi_auroc.update(input=outputs, target=targets)\n",
    "        auroc = multi_auroc.compute()\n",
    "        \n",
    "        \n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_auroc  += (auroc * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_auroc = running_auroc / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, Valid_Auroc=epoch_auroc,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "  \n",
    "    mlflow.log_metric(\"eval_loss\", f\"{epoch_loss:2f}\", step=epoch)\n",
    "    mlflow.log_metric(\"eval_MulticlassAUROC\", f\"{epoch_auroc:2f}\", step=epoch)\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "def run_training(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader, run_id=\"latest\"):\n",
    "    os.makedirs(\"/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main/artifacts/runs/\"+run_id)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_auroc = -np.inf\n",
    "    history = defaultdict(list)\n",
    "    multi_auroc = MulticlassAUROC(num_classes=4,average=None)\n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss, train_epoch_auroc = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss, val_epoch_auroc = valid_one_epoch(model, optimizer,valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        history['Train AUROC'].append(train_epoch_auroc)\n",
    "        history['Valid AUROC'].append(val_epoch_auroc)\n",
    "        history['lr'].append( scheduler.get_lr()[0] )\n",
    "        \n",
    "        # deep copy the model\n",
    "        print(f\"best_epoch_auroc {best_epoch_auroc}\")\n",
    "        print(f\"val_epoch_auroc {val_epoch_auroc}\")\n",
    "        \n",
    "        \n",
    "        if best_epoch_auroc <= val_epoch_auroc:\n",
    "            print(f\"Validation AUROC Improved ({best_epoch_auroc} ---> {val_epoch_auroc})\")\n",
    "            best_epoch_auroc = val_epoch_auroc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"/AUROC{:.4f}_Loss{:.4f}_epoch{:.0f}.bin\".format(val_epoch_auroc, val_epoch_loss, epoch)\n",
    "            torch.save(model.state_dict(), \"/home/vikram/Downloads/pop_os_backup/Kidney-Disease-Classification-Deep-Learning-Project-main/artifacts/\" + run_id + PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved{PATH}\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best AUROC: {:.4f}\".format(best_epoch_auroc))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], \n",
    "                       weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KidneyDataset(X_train, transforms=data_transforms[\"train\"])\n",
    "valid_dataset = KidneyDataset(X_valid, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=1, shuffle=True, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0.3924, 0.7158, 0.5311, 0.0579],\n",
    "        [0.2794, 0.5616, 0.5216, 0.0404],\n",
    "        [0.3344, 0.7559, 0.4294, 0.0984],\n",
    "        [0.2147, 0.6359, 0.5156, 0.0778],\n",
    "        [0.2850, 0.7352, 0.5044, 0.0943],\n",
    "        [0.3265, 0.7074, 0.5846, 0.0850],\n",
    "        [0.2186, 0.8068, 0.4628, 0.0655],\n",
    "        [0.1892, 0.8145, 0.4243, 0.0704],\n",
    "        [0.2459, 0.6203, 0.6948, 0.1244],\n",
    "        [0.1527, 0.7540, 0.5675, 0.0454],\n",
    "        [0.5703, 0.7642, 0.4513, 0.0628],\n",
    "        [0.1669, 0.7222, 0.5182, 0.0436],\n",
    "        [0.4301, 0.7556, 0.7043, 0.1060],\n",
    "        [0.3571, 0.6288, 0.4984, 0.1113],\n",
    "        [0.2755, 0.8199, 0.2628, 0.1294],\n",
    "        [0.3336, 0.6621, 0.4585, 0.1290]])\n",
    "\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.argmax(t, dim=1).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [21:25<00:00,  3.30s/it, Epoch=1, LR=1.24e-5, Train_Auroc=tensor(0.9451), Train_Loss=0.836]\n",
      "100%|██████████| 131/131 [02:09<00:00,  1.01it/s, Epoch=1, LR=1.24e-5, Valid_Auroc=tensor(0.9725), Valid_Loss=0.753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUROC Improved (-inf ---> 0.9725459218025208)\n",
      "Model SavedAUROC0.9725_Loss0.7531_epoch1.bin\n",
      "Training complete in 0h 23m 41s\n",
      "Best AUROC: 0.9725\n"
     ]
    }
   ],
   "source": [
    "#model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              train_loader=train_loader,\n",
    "                              valid_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 86/521 [03:29<17:37,  2.43s/it, Epoch=1, LR=0.000992, Train_Auroc=tensor(0.6129), Train_Loss=1.22]\n",
      "2024/11/05 01:21:31 INFO mlflow.tracking._tracking_service.client: 🏃 View run laptop_run2 at: https://dagshub.com/vicky5848d/Kidney-Disease-Classification.mlflow/#/experiments/0/runs/d50f34b489be48128d8c5b29e081a645.\n",
      "2024/11/05 01:21:31 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/vicky5848d/Kidney-Disease-Classification.mlflow/#/experiments/0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[361], line 45\u001b[0m\n\u001b[1;32m     38\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_input(X_valid_dataset, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlflow.runName\u001b[39m\u001b[38;5;124m'\u001b[39m, run_id)\n\u001b[0;32m---> 45\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Save the trained model to MLflow.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mlog_model(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[354], line 17\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs, train_loader, valid_loader, run_id)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m): \n\u001b[1;32m     16\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 17\u001b[0m     train_epoch_loss, train_epoch_auroc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     val_epoch_loss, val_epoch_auroc \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, optimizer,valid_loader, device\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     22\u001b[0m                                      epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     24\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n",
      "Cell \u001b[0;32mIn[353], line 22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_accumulate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pip install torchinfo\n",
    "from torchinfo import summary\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "run_id = \"laptop_run2\"\n",
    "with mlflow.start_run(run_name=run_id):\n",
    "    params = {\n",
    "        \n",
    "        \n",
    "        \"img_size\": CONFIG['img_size'],\n",
    "        \"model_name\": CONFIG[\"model_name\"],       \n",
    "        \"scheduler\": CONFIG['scheduler'],\n",
    "        \"min_lr\": CONFIG['min_lr'],     \n",
    "        \"n_accumulate\": CONFIG['n_accumulate'],\n",
    "        \"seed\": CONFIG['seed'],\n",
    "        \"epochs\": CONFIG['epochs'],\n",
    "        \"learning_rate\": CONFIG['learning_rate'],\n",
    "        \"train_batch_size\": CONFIG['train_batch_size'],\n",
    "        \"val_batch_size\": CONFIG['valid_batch_size'],\n",
    "        \"loss_function\": \"CrossEntropyLoss\",\n",
    "        \"weight_decay\": CONFIG['weight_decay'],\n",
    "        \"metric_function\": \"MulticlassAUROC\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"Tmax\": CONFIG['T_max'], \n",
    "        \"eta_min\": CONFIG['min_lr']\n",
    "    }\n",
    "    # Log training parameters.\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log model summary.\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "    \n",
    "    X_train_dataset: PandasDataset = mlflow.data.from_pandas(X_train)\n",
    "    X_valid_dataset: PandasDataset = mlflow.data.from_pandas(X_valid)\n",
    "        \n",
    "    mlflow.log_input(X_train_dataset, context=\"training\")\n",
    "    mlflow.log_input(X_valid_dataset, context=\"validation\")\n",
    "    \n",
    "    \n",
    "    mlflow.set_tag('mlflow.runName', run_id)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'],\n",
    "                              train_loader=train_loader,\n",
    "                              valid_loader=valid_loader,\n",
    "                              run_id=run_id)\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "model_load = EffNetModel(CONFIG['model_name'], checkpoint_path=None)\n",
    "state = torch.load(\"/kaggle/working/AUROC0.6751_Loss0.7461_epoch1.bin\", map_location=torch.device('cpu'))\n",
    "model_load.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot( range(history.shape[0]), history[\"Train Loss\"].values, label=\"Train Loss\")\n",
    "plt.plot( range(history.shape[0]), history[\"Valid Loss\"].values, label=\"Valid Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(history.shape[0]), history[\"Train AUROC\"].values, label=\"Train AUROC\")\n",
    "plt.plot( range(history.shape[0]), history[\"Valid AUROC\"].values, label=\"Valid AUROC\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(history.shape[0]), history[\"lr\"].values, label=\"lr\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"lr\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:52:42,695: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-10-02 09:52:42,700: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-10-02 09:52:42,702: INFO: common: created directory at: artifacts]\n",
      "[2023-10-02 09:52:42,703: INFO: common: created directory at: artifacts/prepare_base_model]\n",
      "[2023-10-02 09:52:43,136: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "    prepare_base_model.get_base_model()\n",
    "    prepare_base_model.update_base_model()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /home/vikram/miniconda3/envs/ds/lib/python3.10/site-packages (from torcheval) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheval.metrics import MulticlassAUROC\n",
    "metric = MulticlassAUROC(num_classes=4)\n",
    "input = torch.tensor([[0.1, 0.1, 0.1, 0.1], [0.5, 0.5, 0.5, 0.5], [0.7, 0.7, 0.7, 0.7], [0.8, 0.8, 0.8, 0.8]])\n",
    "target = torch.tensor([0, 1, 2, 3])\n",
    "metric.update(input, target)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8333, 0.3333, 0.6667, 1.0000])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheval.metrics import MulticlassAUROC\n",
    "metric = MulticlassAUROC(num_classes=4, average=None)\n",
    "input = torch.tensor([[0.8, 0.3, 0.1, 0.1], [0.5, 0.6, 0.5, 0.5], [0.7, 0.8, 0.7, 0.7], [0.8, 0.8, 0.96, 0.8]])\n",
    "target = torch.tensor([0, 1, 2, 3])\n",
    "metric.update(input, target)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "step 0 | batch 0\n",
      "step 1 | batch 100\n",
      "step 2 | batch 200\n",
      "step 3 | batch 300\n",
      "step 4 | batch 400\n",
      "step 5 | batch 500\n",
      "step 6 | batch 600\n",
      "step 7 | batch 700\n",
      "step 8 | batch 800\n",
      "step 9 | batch 900\n",
      "EPOCH 1\n",
      "step 0 | batch 0\n",
      "step 2 | batch 100\n",
      "step 4 | batch 200\n",
      "step 6 | batch 300\n",
      "step 8 | batch 400\n",
      "step 10 | batch 500\n",
      "step 12 | batch 600\n",
      "step 14 | batch 700\n",
      "step 16 | batch 800\n",
      "step 18 | batch 900\n",
      "EPOCH 2\n",
      "step 0 | batch 0\n",
      "step 3 | batch 100\n",
      "step 6 | batch 200\n",
      "step 9 | batch 300\n",
      "step 12 | batch 400\n",
      "step 15 | batch 500\n",
      "step 18 | batch 600\n",
      "step 21 | batch 700\n",
      "step 24 | batch 800\n",
      "step 27 | batch 900\n",
      "EPOCH 3\n",
      "step 0 | batch 0\n",
      "step 4 | batch 100\n",
      "step 8 | batch 200\n",
      "step 12 | batch 300\n",
      "step 16 | batch 400\n",
      "step 20 | batch 500\n",
      "step 24 | batch 600\n",
      "step 28 | batch 700\n",
      "step 32 | batch 800\n",
      "step 36 | batch 900\n",
      "EPOCH 4\n",
      "step 0 | batch 0\n",
      "step 5 | batch 100\n",
      "step 10 | batch 200\n",
      "step 15 | batch 300\n",
      "step 20 | batch 400\n",
      "step 25 | batch 500\n",
      "step 30 | batch 600\n",
      "step 35 | batch 700\n",
      "step 40 | batch 800\n",
      "step 45 | batch 900\n"
     ]
    }
   ],
   "source": [
    "e = 5   \n",
    "b = 1000\n",
    "\n",
    "for epoch in range(e):\n",
    "    print(f\"EPOCH {epoch}\")\n",
    "    for bat in range(b):\n",
    "        if bat % 100 == 0:\n",
    "           \n",
    "            step = bat // 100 * (epoch + 1)\n",
    "            print(f\"step {step} | batch {bat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heller, N., Sathianathen, N., Kalapara, A., Walczak, E., Moore, K., Kaluzniak, H., Rosenberg, J., Blake, P., Rengel, Z., Oestreich, M., Dean, J., Tradewell, M., Shah, A., Tejpaul, R., Edgerton, Z., Peterson, M., Raza, S., Regmi, S., Papanikolopoulos, N., Weight, C.  (2019) Data from C4KC-KiTS  [Data set]. The Cancer Imaging Archive. DOI: 10.7937/TCIA.2019.IX49E8NX "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
